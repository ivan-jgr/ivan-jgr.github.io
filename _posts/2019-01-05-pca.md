---
layout: post
title: Análisis de Componentes Principales
date: 2019-01-05

tags: pca backpropagation descomposicion-lineal dimensionalidad
splash_img_source: /assets/img/pca/train_faces.png
splash_img_caption: Arquitectura de VoxelMorph. Tomada del <a href="https://arxiv.org/pdf/1809.05231.pdf">paper original</a>. # Splash image caption

# Optional front matter
#updated: 2021-02-05 # Updated date in YYYY-MM-DD format
author: 
  name: Iván Grijalva # Author name, if not provided defaults to site.author.name
  homepage: http://example.com # Author link, if not provided defaults to site.author.homepage
pin: false # true if this post must be pinned on top of the page, default is false.
listed: false # false if this post must NOT be included on the posts page, sitemap, and any of the tag pages, default is true
index: true # When false, <meta name="robots" content="noindex"> is added to the page, default is true
---

El objetivo del análisis de componentes principales (Principal Component Analysis - PCA) es reducir la dimensionalidad de un dataset manteniendo la mayor variación posible. En términos más formales, buscamos una nueva *base* para *proyectar* el dataset con la esperanza de filtrar el ruido y revelar dinámicas ocultas en los datos.

El hecho de buscar una nueva base que sea combinación lineal de la base original impone una fuerte condición: **linealidad**. Sean $\mathbf{X
}$ y $\mathbf{Y}$ son dos matrices de dimensión $m \times n$ relacionadas mediante una transformación lineal $\mathbf{P}$, digamos que $\mathbf{X}$ es el dataset original y $\mathbf{Y}$ es la nueva representación de este mismo dataset, i.e.:

$$\mathbf{PX} = \mathbf{Y}$$

Con esta notación más precisa, las filas de la matrix $\mathbf{P}$ son nuestros nuevos vectores-base o los llamados componentes principales. Ahora solo nos queda la pregunta ¿Cómo elegir $\mathbf{P}$?. En base a nuestro objetivo inicial queremos que $\mathbf{P}$ exprese de la mejor manera posible el significado de nuestros datos. Es decir, queremos tener la menor reduncia posible (además de la razón entre señal y ruido) que podemos medir a través de la varianza o mas bien a través de la covarianza. Una matriz de covarianza nos permitirá descubrir la relación entre los posibles pares de medidas de nuetro dataset e idealmente quisieramos que la covarianza entre medidas separadas sea cero. Con esto en mente podemos someter la búsqueda de la matriz $\mathbf{P}$ a esta condición.

Para demostrar como el PCA funciona usemos el dataset [Labelles Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) que consiste en 13233 rostros en escala de grises de tamaño 64x64, i.e. cada rostro tiene dimensión 4096.

```python

import numpy as np
import matplotlib.pyplot as plt
import cv2
from os import listdir, path

DATASET_NAME = 'lfwcrop_grey'

filenames = listdir(f'{DATASET_NAME}/faces')
images = []
# Leemos todas las imagenes en el dataset
for img_name in filenames:
    images.append(cv2.imread(f'{DATASET_NAME}/faces/{img_name}', cv2.IMREAD_GRAYSCALE))

images = np.array(images)

# (13233, 64, 64)
print("Dataset shape", images.shape)
# (13233, 4096)
X = images.reshape(-1, 64*64)
```

Algunas imágenes en el dataset:

<figure>
  <img src='/assets/img/pca/sample_faces.png'>
  <figcaption> <p style='text-align:center; color:gray'>Imágenes en el dataset.</p> </figcaption>
</figure>

##### Análisis de Componentes Principales en Numpy

Primero, buscaremos la matriz de covarianza para nuestro dataset.

```python 
# Matrix de covarianza
covariance_matrix = np.cov(X.T)
# Vectores y valores propios
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

eigen_pair = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]

# Ordenamos de acuerdo a la magnitud de los valores propios
eigen_pair.sort(key=lambda x: x[0], reverse=True)

```

La varianza nos dirá cuanta información aporta cada componente, esta varianza está representada por los valores propios (en cada dimensión). Si conservamos demasiadas componentes, la reducción de la dimensionalidad pierde su propósito (porque la dimensionalidad seguirá siendo alta), pero si conservamos muy pocas, las imágenes reconstruidas serán muy pobres. Podemos solucionar esto fijando el porcentaje de la varianza que queremos conservar después de la proyección, de esta forma, la cantidad de componentes (la dimensión de la matriz $\mathbf{P}$) queda determinada por el dataset que tenemos:

```python 
# Queremos conservar el 99% de la varianza
keep_var=0.99

required_variance = keep_variance * sum(eigenvalues)

required_dim = 0 # la dimension de P
variance = 0

for value, _ in eigen_pair:
  variance += value
  if variance >= required_variance:
    required_dim = i + 1
    break

print('Dimensiones originales: {}'.format(len(eigen_pair)))
print('Dimensiones requeridas: {}'.format(required_dim))
```

Ahora ya podemos encontrar nuestra matriz $\mathbf{P}$:


```python 
# La matriz P tiene tantas filas como ejemplos en el dataset y la cantidad de columnas
# determinada anteriormente
projection_matrix = np.empty(shape = (X.shape[1], required_dim))

for i, (_, vector) in enumerate(eigen_pair):
  if i == required_dim:
    break
  
  projection_matrix[:, i] = vector

print(f'Dimensiones de la matriz P: {projection_matrix.shape}')
```

En un abuso de notación prestada del Álgebra Lineal, estas componentes son llamadas eigenfaces (algo así como *caras características o propias*). Todos los ejemplos de dataset se pueden expresar como una combinación lineal de estos elementos.

<figure>
  <img src='/assets/img/pca/eigenfaces.png'>
  <figcaption> <p style='text-align:center; color:gray'> Algunos elementos de la matriz P.</p> </figcaption>
</figure>


Finalmente, una vez obtenida la matrix $\mathbf{P}$ podemos proyectar el dataset.

```python 
# Notar las dimensiones:
# X.shape = (13233, 4096) y projection_matrix.shape = (4096, 577)
projected_data = X.dot(projection_matrix)

```

Nuestro nuevo dataset tiene dimensiones $(13233, 577)$, un porcentaje de compresión de aproximadamente el 81%. Para obtener la imagen original hacemos un producto punto entre la imagen proyectada y la matriz $\mathbf{P}^T$.

```python 

reconstructed_image = projected_image.dot(projection_matrix.T)

```

<figure>
  <center>
    <img src='/assets/img/pca/reconstructed1.png'>
    <img src='/assets/img/pca/reconstructed4.png'>
  </center>
  <figcaption> <p style='text-align:center; color:gray'> Comparación entre una imagen original y una reconstruida.</p> </figcaption>
</figure>